{
	"Flask Upload Image": {
		"prefix": "upload_image",
		"body": [
			"@${1:file_name}.route('/UploadImage', method=[\"POST\"])",
			"def uploadImage():",
			"\tfile = request.files[\"img\"]",
			"\tfile_name = file.filename",
			"",
			"\tfile_path = os.path.join(\"upload\")",
			"\t",
			"\tif not os.path.exists(file_path):",
			"\t\tos.makedirs(file_path)",
			"\t\t",
			"\tpath = os.path.join(file_path, file_name)",
			"\tfile.save(path)",
			"",
			"\tdata = {}",
			"\t",
			"\tdata[\"ResponseCode\"] = \"00\"",
			"\tdata[\"ResponseMessage\"] = \"Yo Piere\"",
			"\treturn data",
			""
		],
		"description": "Flask Upload Image"
	},
	"Flask Init": {
		"prefix": "flask_init",
		"body": [
			"from flask import Flask",
			"from flask_cors import CORS",
			"",
			"import os",
			"",
			"import controller",
			"",
			"app = Flask(__name__)",
			"",
			"# Add Routing",
			"app.register_blueprint(controller.${1:name_bp}, url_prefix=\"/${2:url_name}\")",
			"",
			"@app.route('/', methods=[\"GET\"])",
			"def index():",
			"\tfile_dir = f\"{os.getcwd()}\\public\\index.html\"",
			"\t",
			"\t# Read File",
			"\tinFile = open(file_dir, \"r\")",
			"\t",
			"\tline = inFile.read()",
			"\t",
			"\treturn line",
			"",
			"if __name__ == '__main__':",
			"\tCORS(app)",
			"\tapp.run(debug=True)"
		],
		"description": "Flask Init"
	},
	"Base64 Encode": {
		"prefix": "encode64",
		"body": [
			"def encode64(in_path, out_path):",
			"\tinFile = open(in_path, \"rb\")",
			"\timg = inFile.read()",
			"\tdata = base64.b64encode(img)",
			"\tinFile.close()",
			"",
			"\t# Remove b",
			"\tdata = data.decode('utf-8')",
			"",
			"\t# Write File",
			"\toutFile = open(out_path, \"w\")",
			"\toutFile.write(data)",
			"\toutFile.close()",
			"\treturn"
		],
		"description": "Base64 Encoder"
	},
	"Bse64 Decoder": {
		"prefix": "decode64",
		"body": [
			"def decode64(in_path, out_path):",
			"\tinFile = open(in_path, \"r\")",
			"\tdata = inFile.read()",
			"\tinFile.close()",
			"",
			"\t# Convert From str to Data",
			"\tdata = base64.b64decode(data)",
			"",
			"\t# Write Image",
			"\toutFile = open(out_path, \"wb\")",
			"\toutFile.write(data)",
			"\toutFile.close()",
			"",
			"\treturn"
		],
		"description": "Base64 Decoder"
	},
	"PyTorch Train function": {
		"prefix": "train_pytorch",
		"body": [
			"def train(net, train_loader, optimizer, num_epochs, device=\"cpu\"):",
			"",
			"\t# transfer model to GPU",
			"\tnet.to(device)",
			"",
			"\t# set to training mode",
			"\tnet.train()",
			"",
			"\t# train the network",
			"\tfor e in range(num_epochs):",
			"",
			"\t\trunning_loss = 0.0",
			"",
			"\t\ttotal_len = len(train_loader)",
			"\t\tfor ind, (inputs, labels) in enumerate(train_loader):",
			"",
			"\t\t\t# Clear all the gradient to 0",
			"\t\t\toptimizer.zero_grad()",
			"",
			"\t\t\t# transfer data to GPU",
			"\t\t\tinputs.to(device)",
			"\t\t\tlabels.to(device)",
			"",
			"\t\t\t# forward propagation to get h",
			"\t\t\touts = net(inputs)",
			"",
			"\t\t\t# compute loss",
			"\t\t\tlabels = labels.type(torch.LongTensor)",
			"\t\t\tloss = F.cross_entropy(outs, labels)",
			"",
			"\t\t\t# backpropagation to get dw",
			"\t\t\tloss.backward()",
			"",
			"\t\t\t# update the parameters",
			"\t\t\toptimizer.step()",
			"",
			"\t\t\t# get the loss",
			"\t\t\trunning_loss += loss.item()",
			"",
			"\t\t# compute the averaged loss in each epoch",
			"\t\ttrain_loss = running_loss / total_len",
			"\t\tprint(f'[Epoch {e+1:2d}/{num_epochs:d} Iter {ind+1:5d}/{total_len}]: train_loss = {train_loss:.4f}')",
			"",
			"\treturn"
		],
		"description": "Pytorch Train Function"
	},
	"Pytorch Evaluate Function": {
		"prefix": "eval_pytorch",
		"body": [
			"def evaluate(net, testloader, device = \"cpu\"):",
			"",
			"\t# set to evaluation mode",
			"\tnet.eval()",
			"",
			"\t# running_correct",
			"\trunning_corrects = 0",
			"",
			"\t# Repeat for all batch data in the test set",
			"\tfor inputs, targets in testloader:",
			"",
			"\t\t# transfer to the GPU",
			"\t\tinputs = inputs.to(device)",
			"\t\ttargets = targets.to(device)",
			"",
			"\t\t# # disable gradient computation",
			"\t\twith torch.no_grad():",
			"",
			"\t\t\t# perform inference",
			"\t\t\toutputs = net(inputs)",
			"",
			"\t\t\t# predict as the best result",
			"\t\t\t_, predicted = torch.max(outputs, 1)",
			"",
			"\t\t\trunning_corrects += (targets == predicted).double().sum()",
			"",
			"",
			"\tprint('Accuracy = {:.2f}%'.format(100*running_corrects/len(testloader.dataset)))"
		],
		"description": "Pytorch Evaluate Function"
	},
	"PyTorch Custom Model": {
		"prefix": "model_pytorch",
		"body": [
			"def build_block(in_channel, out_channel):",
			"\tblock = nn.Sequential(",
			"\t\tnn.Linear(in_channel, out_channel),",
			"\t\tnn.BatchNorm1d(out_channel),",
			"\t\tnn.ReLU(),",
			"\t)",
			"\treturn block",
			"",
			"",
			"class Model(nn.Module):",
			"",
			"\tdef __init__(self, num_feature, num_class):",
			"\t\tsuper().__init__()",
			"",
			"\t\tlayer_size = [num_feature, 512, 128, 64, num_class]",
			"",
			"\t\tlinear_ls = [",
			"\t\t\tnn.Linear(in_channel, out_channel)",
			"\t\t\tfor (in_channel, out_channel) in zip(layer_size[:-1], layer_size[1:])",
			"\t\t]",
			"\t\tself.fc = nn.ModuleList(linear_ls)",
			"",
			"\t\tbatch_norm_ls = [nn.BatchNorm1d(channel) for channel in layer_size[1:-1]]",
			"\t\tself.batchNorm = nn.ModuleList(batch_norm_ls)",
			"",
			"\tdef forward(self, x):",
			"",
			"\t\tfor fc, batchNorm in zip(self.fc[:-1], self.batchNorm):",
			"",
			"\t\t\tx = fc(x)",
			"\t\t\tx = batchNorm(x)",
			"\t\t\tx = torch.relu(x)",
			"",
			"\t\tself.fc[-1](x)",
			"",
			"\t\treturn x",
			"",
			"_, num_feat = df_X.shape",
			"num_class = len(df_Y.unique())",
			"",
			"net = Model(num_feat, num_class)",
			"",
			"summary(net, (num_feat,))"
		],
		"description": "Pytorch Custom Model"
	},
	"PyTorch Dataset": {
		"prefix": "dataset_pytorch",
		"body": [
			"from torch.utils.data import Dataset, DataLoader",
			"",
			"class FaceDataset(Dataset):",
			"\tdef __init__(self, X, Y):",
			"\t\tself.X = X.copy().values.astype(np.float32)",
			"\t\tself.y = Y.copy()",
			"",
			"\tdef __len__(self):",
			"\t\treturn len(self.y)",
			"",
			"\tdef __getitem__(self, idx):",
			"",
			"\t\tfeature = self.X[idx]",
			"\t\ttarget = self.y[idx]",
			"",
			"",
			"\t\treturn feature, target"
		],
		"description": "PyTorch Dataset"
	},
	"Pytorch Exec Train Function": {
		"prefix": "train_exec_pytorch",
		"body": [
			"dataset = FaceDataset(df_X, df_Y)",
			"",
			"batch_size = 16",
			"train_loader = DataLoader(dataset, batch_size = batch_size, shuffle = True)",
			"",
			"optimizer = optim.Adam(net.parameters(), lr=0.001)",
			"",
			"train(net, train_loader, optimizer,\t5)"
		],
		"description": "Pytorch Exec Train Function"
	},
	"route": {
		"prefix": "route",
		"body": "@bp.route(\"$1\", methods=[\"$2\"])",
		"description": "Flask Route"
	},
	"Flask Controller": {
		"prefix": "init",
		"body": [
			"from flask import Blueprint, jsonify, request\r",
			"from flask_cors import cross_origin\r",
			"\r",
			"from app.utility import clsLogger\r",
			"\r",
			"bp = Blueprint('$1', __name__)"
		],
		"description": "description for Flask Controller"
	},
	"Flask Get": {
		"prefix": "get",
		"body": [
			"@bp.route(\"/$1\", methods=[\"GET\"])\r",
			"@cross_origin()\r",
			"def $1():\r",
			"    response_code = \"99\"\r",
			"    data = {}\r",
			"\r",
			"    param = request.args.to_dict()\r",
			"    clsLogger.InfoLog(param)\r",
			"\r",
			"    try:\r",
			"        # Get Parameters Here\r",
			"        $2\r",
			"\r",
			"        # Do Logic Here\r",
			"        $3\r",
			"\r",
			"        response_code = \"00\"\r",
			"    except Exception as e:\r",
			"        clsLogger.ErrorLog(e)\r",
			"\r",
			"    data[\"ResponseCode\"] = response_code\r",
			"    return jsonify(data)"
		],
		"description": "description for Flask Get"
	},
	"Flask Post": {
		"prefix": "post",
		"body": [
			"@bp.route(\"/$1\", methods=[\"POST\"])\r",
			"@cross_origin()\r",
			"def $1():\r",
			"    response_code = \"99\"\r",
			"    data = {}\r",
			"\r",
			"    param = request.get_json()\r",
			"    clsLogger.InfoLog(param)\r",
			"\r",
			"    try:\r",
			"        # Get Parameters Here\r",
			"        $2\r",
			"\r",
			"        # Put Logic Here\r",
			"        $3\r",
			"\r",
			"        response_code = \"00\"\r",
			"    except Exception as e:\r",
			"        clsLogger.ErrorLog(e)\r",
			"\r",
			"    data[\"ResponseCode\"] = response_code\r",
			"    return jsonify(data)"
		],
		"description": "description for Flask Post"
	},
	"Basic File IO": {
		"prefix": ["readFile", "json", "writeFile", "output", "file"],
		"body": [
			"#region File IO",
			"def read_json(file_path):",
			"    try:",
			"        with open(file_path, \"r\", encoding=\"utf-8\") as file:",
			"            data = json.load(file)",
			"            # file.readlines(data)",
			"        file.close()",
			"",
			"        print(f\"Successfully read Data from {file_path}!\")",
			"",
			"        return data",
			"    except Exception as ex:",
			"        print(f\"Error! Unable to read Data from {file_path}! Exception: {ex}\")",
			"",
			"def write_json(data, file_path):",
			"    try:",
			"        with open(file_path, \"w\", encoding=\"utf-8\") as file:",
			"            json.dump(data, file)",
			"            # file.write(file)",
			"        file.close()",
			"",
			"        print(f\"Successfully write Data to {file_path}!\")",
			"    except Exception as ex:",
			"        print(f\"Error! Unable to write Data to {file_path}! Exception: {ex}\")",
			"#endregion"
		],
		"description": "Basic File IO"
	},
	"Jupyter Notebook SQL": {
		"prefix": "sql",
		"body": [
			"from sqlalchemy import create_engine",
			"import urllib",
			"",
			"mssql = {",
			"    # \"driver\": \"SQL Server Native Client 11.0\",",
			"    \"driver\": \"ODBC Driver 18 for SQL Server\",",
			"    \"host\": \"47.254.229.107\",",
			"    \"port\": \"1433\",",
			"    \"db\": \"Sandbox_Yatu\",",
			"    \"uid\": \"sa\",",
			"    \"pwd\": \"Nomoneynotalk88!\"",
			"}",
			"",
			"params = urllib.parse.quote_plus( ",
			"    f'Driver={mssql[\"driver\"]};' +",
			"    f'Server=tcp:{mssql[\"host\"]},{mssql[\"port\"]};' +",
			"    f'Database={mssql[\"db\"]};' +",
			"    f'uid={mssql[\"uid\"]};' +",
			"    f'pwd={mssql[\"pwd\"]};' +",
			"    'TrustServerCertificate=yes;'",
			")",
			"conn_str = 'mssql+pyodbc:///?odbc_connect={}'.format(params)",
			"engine = create_engine(conn_str,echo=True)"
		],
		"description": "description for Jupyter Notebook SQL"
	},
	"Region Comments": {
		"prefix": "region",
		"body": [
			"#region ${1:region_name}\r",
			"$TM_SELECTED_TEXT\r",
			"#endregion"
		],
		"description": "description for region"
	},
	"SQL Store Procedure": {
		"prefix": "stmt",
		"body": [
			"def ${func_name}(cursor, ${sp_param}):\r",
			"    query = \"SET NOCOUNT ON; EXEC ${sp_name};\"\r",
			"    values = (${sp_param})\r",
			"\r",
			"    try:\r",
			"        cursor.execute(query, values)\r",
			"        cursor.commit()\r",
			"    except Exception as ex:\r",
			"        clsLogger.ErrorLog(f\"${func_name} {ex}\")\r",
			"        clsLogger.ErrorLog(values)"
		],
		"description": "description for SQL Store Procedure"
	},
	"SQL Select Statment": {
		"prefix": "stmt",
		"body": [
			"def ${func_name}(cursor, ${sp_param}):\r",
			"\r",
			"    query = \"${sp_stmt}\"\r",
			"    values = (${sp_param})\r",
			"\r",
			"    try:\r",
			"        res_ls = cursor.execute(query, values)\r",
			"\r",
			"        def f(arr): return {}\r",
			"        res_ls = [f(obj) for obj in res_ls]\r",
			"\r",
			"        return res_ls\r",
			"    except Exception as ex:\r",
			"        clsLogger.ErrorLog(values)\r",
			"        clsLogger.ErrorLog(ex)\r",
			"\r",
			"    return []"
		],
		"description": "description for SQL Select Statment"
	},
	"try-catch": {
		"prefix": "try",
		"body": [
			"try:",
			"    $TM_SELECTED_TEXT",
			"except Exception as ex:",
			"    print(f\"Exception: {ex}\")"
		],
		"description": "description for try-catch"
	},
	"Store Procedure Boolean": {
		"prefix": "store_procedure",
		"body": [
			"def ${STORE_PROCEDURE}(cursor, code):",
			"    query = \"SET NOCOUNT ON; EXEC ${STORE_PROCEDURE} ?;\"",
			"    values = (code)",
			"",
			"    try:",
			"        cursor.execute(query, values)",
			"        cursor.commit()",
			"    except Exception as e:",
			"        clsLogger.ErrorLog(values)",
			"        clsLogger.ErrorLog(e)"
		],
		"description": "description for Store Procedure Sample"
	},
	"Store Procedure Select": {
		"prefix": "store_procedure",
		"body": [
			"def ${STORE_PROCEDURE}(cursor, home_id):",
			"",
			"    query = \"SET NOCOUNT ON; EXEC ${STORE_PROCEDURE} ?;\"",
			"    values = (home_id)",
			"",
			"    try:",
			"        res_ls = cursor.execute(query, values)",
			"",
			"        def f(arr): return { \"device_id\": arr[0], \"device_name\": arr[1], \"sql_id\": arr[2], \"is_temp_humd\": arr[3] }",
			"        res_ls = [f(obj) for obj in res_ls]",
			"",
			"        return res_ls",
			"    except Exception as ex:",
			"        clsLogger.ErrorLog(values)",
			"        clsLogger.ErrorLog(ex)",
			"",
			"    return []"
		],
		"description": "description for Store Procedure Select"
	},
	"euler": {
		"prefix": "euler",
		"body": [
			"import time",
			"",
			"def main():",
			"",
			"    start_time = time.time()",
			"",
			"    print(\"Hello, World!\")",
			"",
			"    end_time = time.time() - start_time",
			"    print(f\"Time Taken: {end_time:.2f} seconds\")",
			"",
			"if __name__ == \"__main__\":",
			"    main()"
		],
		"description": "Project Euler"
	}
}